{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images to RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_images(df):\n",
    "  for i, row in df.iterrows():\n",
    "    image = cv2.imdecode(np.frombuffer(row['image'], np.uint8), cv2.IMREAD_COLOR)\n",
    "    cv2.imwrite(f'./easy-500/images/{i}.jpg', image)\n",
    "\n",
    "images = pd.read_parquet('easy-500/images.parquet')\n",
    "annotations = pd.read_parquet('easy-500/labels.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_id    x    y  orientation  radius  class\n",
      "0         0  269  450     0.000000      17      0\n",
      "1         0  533  299     0.663225      45      1\n",
      "   id                                              image\n",
      "0   0  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
      "1   1  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n"
     ]
    }
   ],
   "source": [
    "print(annotations[:2])\n",
    "print(images[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO simplified Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsymmetricYOLO(nn.Module):\n",
    "    def __init__(self, W=32, H=16, B=3, C=3):\n",
    "        super(AsymmetricYOLO, self).__init__()\n",
    "        self.W = W # Width of Grid\n",
    "        self.H = H # Height of Grid\n",
    "        self.B = B # How many boxes in Grid Cell to search\n",
    "        self.C = C # Amount of Classes (Red Player, Blue Player, Ball)\n",
    "        self.BoxSize = 5\n",
    "        \n",
    "        self.input_channels = 3 # The number of input channels. For RGB images, this is 3.\n",
    "        self.output_channels = 16 # The number of output channels. This layer will create 16 filters, each producing a separate output channel.\n",
    "        self.kernel_size = 3 # The size of the filter applied to the input image. A size of 3 means a 3x3 filter.\n",
    "        self.stride = 1 # The stride of the convolution. A stride of 1 means the filter moves one pixel at a time as it slides across the image.\n",
    "        \n",
    "        # Padding added to the edges of the input. A padding of 1 adds a one-pixel border of zeros around the input image, \n",
    "        # allowing the output size to be the same as the input size when using a stride of 1 and a 3x3 kernel.\n",
    "        self.padding = 1   \n",
    "        \n",
    "        # Define the CNN architecture\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                self.input_channels, \n",
    "                self.output_channels, \n",
    "                kernel_size=self.kernel_size, \n",
    "                stride=self.stride, \n",
    "                padding=self.padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.output_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Add more convolutional layers as needed\n",
    "        )\n",
    "\n",
    "        # Assuming the feature map size here is compatible with the input image size\n",
    "        feature_size = 512 * 256 * self.output_channels\n",
    "        # and grid size after the convolutions and pooling\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_size, W * H * (C + B * self.BoxSize)),  # Adjust 'feature_size' based on the output of the last conv layer\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = x.view(-1, self.H, self.W, self.C + self.B * self.BoxSize)  # Reshape to match the grid size and predictions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SportsDataset(Dataset):\n",
    "    def __init__(self, images, annotations, device):\n",
    "        self.images = images\n",
    "        self.annotations = annotations\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def decodeAnnotation(self, annotation):\n",
    "        print(annotation)\n",
    "        bboxes          = []\n",
    "        classes         = []\n",
    "        orientations    = []\n",
    "\n",
    "        \n",
    "        for object in annotation:\n",
    "            bboxes.append([\n",
    "               object['x'],\n",
    "               object['y'],\n",
    "               object['radius']\n",
    "            ]) \n",
    "           \n",
    "            classes.append(object['class'])\n",
    "            orientations.append(object['orientation'])\n",
    "\n",
    "        return {\n",
    "            'bboxes': bboxes, \n",
    "            'classes': classes, \n",
    "            'orientations': orientations\n",
    "        }\n",
    "    \n",
    "    def normalization(self, image):\n",
    "        \n",
    "         # Normalize pixel values\n",
    "        image   = image / 255.0  # Scale pixels to [0, 1]\n",
    "        \n",
    "        # Manually normalize the image using specified means and stds\n",
    "        mean    = np.array([0.485, 0.456, 0.406])\n",
    "        std     = np.array([0.229, 0.224, 0.225])\n",
    "        image   = (image - mean) / std  # Apply normalization\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images_row = self.images.iloc[idx]\n",
    "        annotation = self.annotations.iloc[idx]\n",
    "        \n",
    "        # Decode image from binary data\n",
    "        image = cv2.imdecode(np.frombuffer(images_row['image'], np.uint8), cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        image = self.normalization(image)\n",
    "\n",
    "        decodedAnnotation = self.decodeAnnotation(annotation)\n",
    "\n",
    "        return {\n",
    "           'image':         torch.tensor(image, dtype=torch.float32, device = self.device).permute(2, 0, 1),\n",
    "           'bboxes':        torch.tensor(decodedAnnotation['bboxes'], dtype=torch.float32, device = self.device), \n",
    "           'classes':       torch.tensor(decodedAnnotation['classes'], dtype=torch.int64, device = self.device), \n",
    "           'orientations':  torch.tensor(decodedAnnotation['orientations'], dtype=torch.float32, device = self.device),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SportsDataset' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 5\u001b[0m ImagesDataset \u001b[38;5;241m=\u001b[39m \u001b[43mSportsDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m ImagesDataLoader \u001b[38;5;241m=\u001b[39m DataLoader(ImagesDataset, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'SportsDataset' object is not callable"
     ]
    }
   ],
   "source": [
    "batch_size=2\n",
    "lr=0.001\n",
    "epochs = 1\n",
    "\n",
    "ImagesDataset = SportsDataset(images, annotations, device)\n",
    "ImagesDataLoader = DataLoader(ImagesDataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# for batch in DataLoader:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     print(batch['image'].shape)        \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     print(batch['bboxes'].shape)    \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     print(batch['classes'].shape) \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     print(batch['orientations'].shape) \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 53\u001b[0m, in \u001b[0;36mSportsDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)  \u001b[38;5;66;03m# Convert BGR to RGB\u001b[39;00m\n\u001b[1;32m     51\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization(image)\n\u001b[0;32m---> 53\u001b[0m decodedAnnotation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecodeAnnotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You may need to adjust this depending on how bboxes are stored\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     56\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m:         torch\u001b[38;5;241m.\u001b[39mtensor(image, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     57\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m:        torch\u001b[38;5;241m.\u001b[39mtensor(decodedAnnotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), \n\u001b[1;32m     58\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m'\u001b[39m:       torch\u001b[38;5;241m.\u001b[39mtensor(decodedAnnotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), \n\u001b[1;32m     59\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morientations\u001b[39m\u001b[38;5;124m'\u001b[39m:  torch\u001b[38;5;241m.\u001b[39mtensor(decodedAnnotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morientations\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m     60\u001b[0m }\n",
      "Cell \u001b[0;32mIn[28], line 17\u001b[0m, in \u001b[0;36mSportsDataset.decodeAnnotation\u001b[0;34m(self, annotation)\u001b[0m\n\u001b[1;32m     13\u001b[0m orientations    \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01min\u001b[39;00m annotation:\n\u001b[1;32m     16\u001b[0m     bboxes\u001b[38;5;241m.\u001b[39mappend([\n\u001b[0;32m---> 17\u001b[0m        \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     18\u001b[0m        \u001b[38;5;28mobject\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m        \u001b[38;5;28mobject\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mradius\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m     ]) \n\u001b[1;32m     22\u001b[0m     classes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mobject\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m     orientations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mobject\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(Dataset.__getitem__(0))\n",
    "\n",
    "# for batch in DataLoader:\n",
    "#     print(batch['image'].shape)        \n",
    "#     print(batch['bboxes'].shape)    \n",
    "#     print(batch['classes'].shape) \n",
    "#     print(batch['orientations'].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Yolo = Yolo(embedding_size, hidden_size, device)\n",
    "Yolo.to(device)\n",
    "optimizer = torch.optim.Adam(Yolo.parameters(), lr)\n",
    "loss_function = nn.MSELoss()  # Simplified loss function for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in DataLoader:\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(Yolo.parameters(), lr)\n",
    "loss_function = nn.MSELoss()  # Simplified loss function for illustration\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in data_loader:\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
