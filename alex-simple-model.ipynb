{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images to RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_images(df):\n",
    "  for i, row in df.iterrows():\n",
    "    image = cv2.imdecode(np.frombuffer(row['image'], np.uint8), cv2.IMREAD_COLOR)\n",
    "    cv2.imwrite(f'./easy-500/images/{i}.jpg', image)\n",
    "\n",
    "images = pd.read_parquet('easy-500/images.parquet')\n",
    "annotations = pd.read_parquet('easy-500/labels.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_id    x    y  orientation  radius  class\n",
      "0         0  269  450     0.000000      17      0\n",
      "1         0  533  299     0.663225      45      1\n",
      "(500, 2)\n"
     ]
    }
   ],
   "source": [
    "# Group by index and aggregate into lists\n",
    "# t1 = annotations.set_index('image_id')\n",
    "# aggregated_annotations = t1.groupby(t1.index).agg(list)\n",
    "\n",
    "# print(aggregated_annotations.shape)\n",
    "print(annotations[:2])\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO simplified Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsymmetricYOLO(nn.Module):\n",
    "    def __init__(self, imageWidth = 1024, imageHeight = 512, W=32, H=16, B=3, C=3, device = device):\n",
    "        super(AsymmetricYOLO, self).__init__()\n",
    "        self.device     = device\n",
    "        self.imageWidth = imageWidth\n",
    "        self.imageHeight= imageHeight\n",
    "        self.W          = W # Width of Grid\n",
    "        self.H          = H # Height of Grid\n",
    "        self.B          = B # How many boxes in Grid Cell to search\n",
    "        self.C          = C # Amount of Classes (Red Player, Blue Player, Ball)\n",
    "        self.BoxSize    = 5\n",
    "        \n",
    "        self.input_channels     = 3     # The number of input channels. For RGB images, this is 3.\n",
    "        self.output_channels    = 16    # The number of output channels. This layer will create 16 filters, each producing a separate output channel.\n",
    "        self.kernel_size        = 21     # The size of the filter applied to the input image. A size of 3 means a 3x3 filter.\n",
    "        self.stride             = 7    # The stride of the convolution. A stride of 1 means the filter moves one pixel at a time as it slides across the image.\n",
    "        \n",
    "        # Padding added to the edges of the input. A padding of 1 adds a one-pixel border of zeros around the input image, \n",
    "        # allowing the output size to be the same as the input size when using a stride of 1 and a 3x3 kernel.\n",
    "        self.padding            = 0   \n",
    "        \n",
    "        # Define the CNN architecture\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                self.input_channels, \n",
    "                self.output_channels, \n",
    "                kernel_size=self.kernel_size, \n",
    "                stride=self.stride, \n",
    "                padding=self.padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.output_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Add more convolutional layers as needed\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Assuming the feature map size here is compatible with the input image size\n",
    "        # feature_size = 512 * 256 * self.output_channels\n",
    "        # and grid size after the convolutions and pooling\n",
    "\n",
    "        # Temporarily forward a dummy input through the conv_layers to find feature_size\n",
    "        dummy_input = torch.autograd.Variable(torch.rand(1, 3, 1024, 512)).to(self.device)\n",
    "        output_feat = self.conv_layers(dummy_input)\n",
    "        feature_size = int(np.prod(output_feat.size()[1:]))  # Multiply dimensions for feature size\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_size, W * H * (C + B * self.BoxSize)),  # Adjust 'feature_size' based on the output of the last conv layer\n",
    "            nn.LeakyReLU(0.1),\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.conv_layers(x)\n",
    "        # x = self.output_layer(x)\n",
    "        # x = x.view(-1, self.H, self.W, self.C + self.B * self.BoxSize)  # Reshape to match the grid size and predictions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SportsDataset(Dataset):\n",
    "    def __init__(self, images, annotations, device):\n",
    "        self.images = images\n",
    "        self.annotations = annotations.set_index('image_id')\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def decodeAnnotation(self, annotation):\n",
    "        bboxes          = []\n",
    "        classes         = []\n",
    "        orientations    = []\n",
    "\n",
    "        \n",
    "        for _, object in annotation.iterrows():\n",
    "            bboxes.append([\n",
    "               object['x'],\n",
    "               object['y'],\n",
    "               object['radius']\n",
    "            ]) \n",
    "           \n",
    "            classes.append(object['class'])\n",
    "            orientations.append(object['orientation'])\n",
    "\n",
    "        return {\n",
    "            'bboxes': bboxes, \n",
    "            'classes': classes, \n",
    "            'orientations': orientations\n",
    "        }\n",
    "    \n",
    "    def normalization(self, image):\n",
    "        \n",
    "         # Normalize pixel values\n",
    "        image   = image / 255.0  # Scale pixels to [0, 1]\n",
    "        \n",
    "        # Manually normalize the image using specified means and stds\n",
    "        mean    = np.array([0.485, 0.456, 0.406])\n",
    "        std     = np.array([0.229, 0.224, 0.225])\n",
    "        image   = (image - mean) / std  # Apply normalization\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images_row = self.images.iloc[idx]\n",
    "        annotations = self.annotations.loc[idx]\n",
    "        \n",
    "        # Decode image from binary data\n",
    "        image = cv2.imdecode(np.frombuffer(images_row['image'], np.uint8), cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        image = self.normalization(image)\n",
    "\n",
    "        decodedAnnotation = self.decodeAnnotation(annotations)\n",
    "\n",
    "        images = torch.tensor(image, dtype=torch.float32, device = self.device).permute(2, 0, 1)\n",
    "        annotations = {\n",
    "           'bboxes':        torch.tensor(decodedAnnotation['bboxes'], dtype=torch.float32, device = self.device), \n",
    "           'classes':       torch.tensor(decodedAnnotation['classes'], dtype=torch.int64, device = self.device), \n",
    "           'orientations':  torch.tensor(decodedAnnotation['orientations'], dtype=torch.float32, device = self.device),\n",
    "        }\n",
    "\n",
    "        return images, annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "lr=0.001\n",
    "epochs = 1\n",
    "\n",
    "ImagesDataset = SportsDataset(images, annotations, device)\n",
    "ImagesDataLoader = DataLoader(ImagesDataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Yolo = AsymmetricYOLO(\n",
    "    imageWidth = 1024,\n",
    "    imageHeight = 512,\n",
    "    W=32, \n",
    "    H=16, \n",
    "    B=3, \n",
    "    C=3, \n",
    "    device = device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(Yolo.parameters(), lr)\n",
    "loss_function = nn.MSELoss()  # Simplified loss function for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m Yolo(images)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# loss = loss_function(outputs, annotations)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# optimizer.zero_grad()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# optimizer.step()\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, annotations in ImagesDataLoader:\n",
    "        outputs = Yolo(images)\n",
    "        # loss = loss_function(outputs, annotations)\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.1418, -1.1247, -1.1247,  ..., -0.8335, -0.8335, -0.8507],\n",
      "          [-1.1075, -1.1247, -1.1418,  ..., -0.8507, -0.8507, -0.8678],\n",
      "          [-1.0904, -1.1075, -1.1418,  ..., -0.8507, -0.8335, -0.8507],\n",
      "          ...,\n",
      "          [-1.3815, -1.3473, -1.3644,  ..., -1.1418, -1.1589, -1.1075],\n",
      "          [-1.3815, -1.3644, -1.3644,  ..., -1.1589, -1.1589, -1.1075],\n",
      "          [-1.4158, -1.3815, -1.3987,  ..., -1.1760, -1.1760, -1.1075]],\n",
      "\n",
      "         [[ 0.1176,  0.1352,  0.1352,  ...,  0.1702,  0.1702,  0.1527],\n",
      "          [ 0.1527,  0.1352,  0.1176,  ...,  0.1527,  0.1527,  0.1352],\n",
      "          [ 0.1702,  0.1527,  0.1176,  ...,  0.1527,  0.1702,  0.1527],\n",
      "          ...,\n",
      "          [ 0.0826,  0.1176,  0.1001,  ...,  0.1176,  0.1001,  0.1527],\n",
      "          [ 0.0826,  0.1001,  0.1001,  ...,  0.1001,  0.1001,  0.1527],\n",
      "          [ 0.0476,  0.0826,  0.0651,  ...,  0.0826,  0.0826,  0.1527]],\n",
      "\n",
      "         [[-1.5081, -1.4907, -1.4907,  ..., -1.3687, -1.3687, -1.3861],\n",
      "          [-1.4733, -1.4907, -1.5081,  ..., -1.3861, -1.3861, -1.4036],\n",
      "          [-1.4559, -1.4733, -1.5081,  ..., -1.3861, -1.3687, -1.3861],\n",
      "          ...,\n",
      "          [-1.6476, -1.6127, -1.6302,  ..., -1.5430, -1.5604, -1.5081],\n",
      "          [-1.6476, -1.6302, -1.6302,  ..., -1.5604, -1.5604, -1.5081],\n",
      "          [-1.6824, -1.6476, -1.6650,  ..., -1.5779, -1.5779, -1.5081]]],\n",
      "\n",
      "\n",
      "        [[[-1.1418, -1.1247, -1.1247,  ..., -0.8335, -0.8335, -0.8507],\n",
      "          [-1.1075, -1.1247, -1.1418,  ..., -0.8507, -0.8507, -0.8678],\n",
      "          [-1.0904, -1.1075, -1.1418,  ..., -0.8507, -0.8335, -0.8507],\n",
      "          ...,\n",
      "          [-1.3815, -1.3473, -1.3644,  ..., -1.1418, -1.1589, -1.1075],\n",
      "          [-1.3815, -1.3644, -1.3644,  ..., -1.1589, -1.1589, -1.1075],\n",
      "          [-1.4158, -1.3815, -1.3987,  ..., -1.1760, -1.1760, -1.1075]],\n",
      "\n",
      "         [[ 0.1176,  0.1352,  0.1352,  ...,  0.1702,  0.1702,  0.1527],\n",
      "          [ 0.1527,  0.1352,  0.1176,  ...,  0.1527,  0.1527,  0.1352],\n",
      "          [ 0.1702,  0.1527,  0.1176,  ...,  0.1527,  0.1702,  0.1527],\n",
      "          ...,\n",
      "          [ 0.0826,  0.1176,  0.1001,  ...,  0.1176,  0.1001,  0.1527],\n",
      "          [ 0.0826,  0.1001,  0.1001,  ...,  0.1001,  0.1001,  0.1527],\n",
      "          [ 0.0476,  0.0826,  0.0651,  ...,  0.0826,  0.0826,  0.1527]],\n",
      "\n",
      "         [[-1.5081, -1.4907, -1.4907,  ..., -1.3687, -1.3687, -1.3861],\n",
      "          [-1.4733, -1.4907, -1.5081,  ..., -1.3861, -1.3861, -1.4036],\n",
      "          [-1.4559, -1.4733, -1.5081,  ..., -1.3861, -1.3687, -1.3861],\n",
      "          ...,\n",
      "          [-1.6476, -1.6127, -1.6302,  ..., -1.5430, -1.5604, -1.5081],\n",
      "          [-1.6476, -1.6302, -1.6302,  ..., -1.5604, -1.5604, -1.5081],\n",
      "          [-1.6824, -1.6476, -1.6650,  ..., -1.5779, -1.5779, -1.5081]]]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(Yolo.parameters(), lr)\n",
    "loss_function = nn.MSELoss()  # Simplified loss function for illustration\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in data_loader:\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
